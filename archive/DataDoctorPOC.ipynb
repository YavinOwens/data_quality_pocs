{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faker is used to generate fake data for testing and development purposes. This isnt necessary for the given task, but it can be useful for creating a realistic and consistent dataset for testing purposes for data quality and reliability checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: Faker in ./.venv/lib/python3.10/site-packages (26.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy data for poc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generated successfully!\n",
      "Successfully read data/hospitals.csv\n",
      "  hospital_id                      name  \\\n",
      "0          H1  Navarro-Hoffman Hospital   \n",
      "1          H2  Cherry and Sons Hospital   \n",
      "2          H3      Hughes-Ward Hospital   \n",
      "3          H4  Morrison-Hansen Hospital   \n",
      "\n",
      "                                            location  \n",
      "0  83613 Victoria Circles Suite 899\\nNew Cynthiap...  \n",
      "1            1744 Gilmore Union\\nMariaside, CA 31212  \n",
      "2              838 Walker Rapids\\nMccoyton, SD 12554  \n",
      "3         2123 Johnson Mount\\nNorth Steven, CA 93615  \n",
      "Successfully read data/clinics.csv\n",
      "  clinic_id                    name  \\\n",
      "0        C1        Hall-Hall Clinic   \n",
      "1        C2       Bell Group Clinic   \n",
      "2        C3        Olson PLC Clinic   \n",
      "3        C4  Miller and Sons Clinic   \n",
      "\n",
      "                                            location hospital_id  \n",
      "0  5060 Amber Mountains\\nEast Brittanymouth, IL 2...          H6  \n",
      "1           267 Mayo Course\\nHernandezstad, KY 17298         H10  \n",
      "2                   PSC 0912, Box 4668\\nAPO AP 09621          H3  \n",
      "3           6567 Ethan Land\\nNew Tracyside, CA 81346         H10  \n",
      "Successfully read data/providers.csv\n",
      "  provider_id             name        specialization     associated_with  \\\n",
      "0          P1  Shannon Russell  General Practitioner            Hospital   \n",
      "1          P2    Casey Vasquez    Social Care Worker  Social Care Agency   \n",
      "2          P3    Craig Beltran          Pediatrician  Social Care Agency   \n",
      "3          P4     Nicole Smith          Pediatrician  Social Care Agency   \n",
      "\n",
      "               organization hospital_id clinic_id  \n",
      "0  Williams, Shaw and Smith          H4       NaN  \n",
      "1                  Hall Ltd          H8       C23  \n",
      "2              Carter-Jones          H5       NaN  \n",
      "3               Pacheco LLC          H5       NaN  \n",
      "Column hospital_id in data/providers.csv converted to string\n",
      "Column clinic_id in data/providers.csv converted to string\n",
      "Successfully read data/patients.csv\n",
      "  patient_id           name date_of_birth  \\\n",
      "0       PAT1     John Young    1991-04-21   \n",
      "1       PAT2  Vanessa Lopez    1961-08-30   \n",
      "2       PAT3   Jennifer Lee    2004-01-28   \n",
      "3       PAT4   Shawn Martin    1988-12-22   \n",
      "\n",
      "                                             address                 phone  \\\n",
      "0  990 Alexander Port Suite 676\\nFostershire, CT ...  001-842-674-1897x331   \n",
      "1             020 Gary Mission\\nLake Kevin, UT 59179     (506)500-7011x081   \n",
      "2           0867 Cortez Cape\\nEast William, MD 79466       +1-604-808-7381   \n",
      "3  025 Patricia Brooks Apt. 382\\nWest Alexander, ...    605.210.1717x91969   \n",
      "\n",
      "                       email assigned_provider_id assigned_hospital_id  \n",
      "0   rachaelfloyd@example.org                  P72                  NaN  \n",
      "1       sandra40@example.org                  P27                  NaN  \n",
      "2  holmesraymond@example.com                  P60                   H1  \n",
      "3         dhicks@example.com                  P83                  NaN  \n",
      "Column assigned_hospital_id in data/patients.csv converted to string\n",
      "Successfully read data/appointments.csv\n",
      "  appointment_id patient_id provider_id            appointment_date  \\\n",
      "0          APPT1     PAT119         P18  2024-05-02 13:20:26.704780   \n",
      "1          APPT2     PAT182         P67  2024-06-11 17:10:21.847651   \n",
      "2          APPT3      PAT87         P98  2024-04-19 14:44:11.557530   \n",
      "3          APPT4     PAT143         P46  2024-05-30 11:49:10.457978   \n",
      "\n",
      "      reason  waiting  waiting_time  \n",
      "0    Checkup    False             0  \n",
      "1    Checkup     True            10  \n",
      "2  Emergency     True            44  \n",
      "3    Checkup     True            71  \n",
      "Successfully read data/medical_assets.csv\n",
      "  asset_id           type                   manufacturer       model  \\\n",
      "0       A1  X-Ray Machine  Cantrell, Robertson and Munoz    occur485   \n",
      "1       A2     CT Scanner                    Lopez Group      add444   \n",
      "2       A3  Defibrillator                 Melendez Group     best313   \n",
      "3       A4     CT Scanner                   Gonzalez Ltd  through202   \n",
      "\n",
      "  installation_date             status hospital_id clinic_id  \n",
      "0        2020-05-06  Under Maintenance          H3        C1  \n",
      "1        2020-12-22        Operational          H2       C15  \n",
      "2        2023-03-13  Under Maintenance          H6       C18  \n",
      "3        2020-02-29        Operational          H2       C21  \n",
      "Successfully read data/workforce.csv\n",
      "  employee_id            name position hospital_id  salary   hire_date\n",
      "0          E1     Walter Todd   Doctor          H2   61164  2021-10-07\n",
      "1          E2  Keith Anderson   Doctor          H1   22396  2023-01-03\n",
      "2          E3   Mark Mitchell   Doctor          H9   52509  2022-10-28\n",
      "3          E4   Tamara Newman   Doctor          H1   57646  2022-01-17\n",
      "Successfully read data/case_allocations.csv\n",
      "  case_id patient_id provider_id  \\\n",
      "0      C1     PAT122         P93   \n",
      "1      C2      PAT66         P90   \n",
      "2      C3      PAT38         P95   \n",
      "3      C4     PAT118         P59   \n",
      "\n",
      "                                    case_description case_status  \n",
      "0  Nearly require dream world among commercial co...     Pending  \n",
      "1  Protect agency hold security really. Green pla...      Closed  \n",
      "2  Success drive TV head whatever sister. Yes mil...      Closed  \n",
      "3  Recent water effect coach understand few. Capi...      Closed  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Data generation functions\n",
    "def generate_hospitals(num_hospitals):\n",
    "    hospitals = []\n",
    "    for i in range(num_hospitals):\n",
    "        hospitals.append({\n",
    "            'hospital_id': f'H{i+1}',\n",
    "            'name': fake.company() + ' Hospital',\n",
    "            'location': fake.address()\n",
    "        })\n",
    "    return pd.DataFrame(hospitals)\n",
    "\n",
    "def generate_clinics(num_clinics, num_hospitals):\n",
    "    clinics = []\n",
    "    for i in range(num_clinics):\n",
    "        clinics.append({\n",
    "            'clinic_id': f'C{i+1}',\n",
    "            'name': fake.company() + ' Clinic',\n",
    "            'location': fake.address(),\n",
    "            'hospital_id': f'H{np.random.randint(1, num_hospitals + 1)}'\n",
    "        })\n",
    "    return pd.DataFrame(clinics)\n",
    "\n",
    "def generate_providers(num_providers, num_hospitals, num_clinics):\n",
    "    providers = []\n",
    "    for i in range(num_providers):\n",
    "        providers.append({\n",
    "            'provider_id': f'P{i+1}',\n",
    "            'name': fake.name(),\n",
    "            'specialization': np.random.choice(['General Practitioner', 'Cardiologist', 'Dermatologist', 'Neurologist', 'Pediatrician', 'Social Care Worker', 'Bank Staff']),\n",
    "            'associated_with': np.random.choice(['Hospital', 'Provider Company', 'Social Care Agency']),\n",
    "            'organization': fake.company(),\n",
    "            'hospital_id': f'H{np.random.randint(1, num_hospitals + 1)}' if np.random.choice([True, False]) else None,\n",
    "            'clinic_id': f'C{np.random.randint(1, num_clinics + 1)}' if np.random.choice([True, False]) else None\n",
    "        })\n",
    "    return pd.DataFrame(providers)\n",
    "\n",
    "def generate_patients(num_patients, providers):\n",
    "    patients = []\n",
    "    provider_ids = providers['provider_id'].tolist()\n",
    "    hospital_ids = providers['hospital_id'].tolist()\n",
    "    for i in range(num_patients):\n",
    "        assigned_provider = np.random.choice(provider_ids)\n",
    "        assigned_hospital = providers.loc[providers['provider_id'] == assigned_provider, 'hospital_id'].values[0]\n",
    "        patients.append({\n",
    "            'patient_id': f'PAT{i+1}',\n",
    "            'name': fake.name(),\n",
    "            'date_of_birth': fake.date_of_birth(minimum_age=0, maximum_age=90),\n",
    "            'address': fake.address(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'email': fake.email(),\n",
    "            'assigned_provider_id': assigned_provider,\n",
    "            'assigned_hospital_id': assigned_hospital\n",
    "        })\n",
    "    return pd.DataFrame(patients)\n",
    "\n",
    "def generate_appointments(num_appointments, providers, patients):\n",
    "    appointments = []\n",
    "    for i in range(num_appointments):\n",
    "        appointment_date = fake.date_time_this_year()\n",
    "        waiting = np.random.choice([True, False])\n",
    "        appointments.append({\n",
    "            'appointment_id': f'APPT{i+1}',\n",
    "            'patient_id': f'PAT{np.random.randint(1, len(patients) + 1)}',\n",
    "            'provider_id': f'P{np.random.randint(1, len(providers) + 1)}',\n",
    "            'appointment_date': appointment_date,\n",
    "            'reason': np.random.choice(['Checkup', 'Follow-up', 'Consultation', 'Emergency']),\n",
    "            'waiting': waiting,\n",
    "            'waiting_time': np.random.randint(0, 120) if waiting else 0\n",
    "        })\n",
    "    return pd.DataFrame(appointments)\n",
    "\n",
    "def generate_medical_assets(num_assets, num_hospitals, num_clinics):\n",
    "    assets = []\n",
    "    for i in range(num_assets):\n",
    "        assets.append({\n",
    "            'asset_id': f'A{i+1}',\n",
    "            'type': np.random.choice(['MRI Machine', 'X-Ray Machine', 'Ultrasound Machine', 'CT Scanner', 'Defibrillator']),\n",
    "            'manufacturer': fake.company(),\n",
    "            'model': fake.word() + str(np.random.randint(100, 999)),\n",
    "            'installation_date': fake.date_this_decade(),\n",
    "            'status': np.random.choice(['Operational', 'Under Maintenance', 'Out of Service']),\n",
    "            'hospital_id': f'H{np.random.randint(1, num_hospitals + 1)}',\n",
    "            'clinic_id': f'C{np.random.randint(1, num_clinics + 1)}'\n",
    "        })\n",
    "    return pd.DataFrame(assets)\n",
    "\n",
    "def generate_workforce(num_employees, num_hospitals):\n",
    "    workforce = []\n",
    "    for i in range(num_employees):\n",
    "        workforce.append({\n",
    "            'employee_id': f'E{i+1}',\n",
    "            'name': fake.name(),\n",
    "            'position': np.random.choice(['Nurse', 'Doctor', 'Technician', 'Administrative Staff']),\n",
    "            'hospital_id': f'H{np.random.randint(1, num_hospitals + 1)}',\n",
    "            'salary': fake.random_number(digits=5, fix_len=True),\n",
    "            'hire_date': fake.date_this_decade()\n",
    "        })\n",
    "    return pd.DataFrame(workforce)\n",
    "\n",
    "def generate_case_allocations(num_cases, providers, patients):\n",
    "    case_allocations = []\n",
    "    for i in range(num_cases):\n",
    "        case_allocations.append({\n",
    "            'case_id': f'C{i+1}',\n",
    "            'patient_id': f'PAT{np.random.randint(1, len(patients) + 1)}',\n",
    "            'provider_id': f'P{np.random.randint(1, len(providers) + 1)}',\n",
    "            'case_description': fake.text(max_nb_chars=200),\n",
    "            'case_status': np.random.choice(['Open', 'Closed', 'Pending'])\n",
    "        })\n",
    "    return pd.DataFrame(case_allocations)\n",
    "\n",
    "def generate_data():\n",
    "    num_hospitals = 10\n",
    "    num_clinics = 30\n",
    "    num_providers = 100\n",
    "    num_patients = 200\n",
    "    num_appointments = 500\n",
    "    num_assets = 50\n",
    "    num_employees = 100\n",
    "    num_cases = 200\n",
    "    \n",
    "    hospitals = generate_hospitals(num_hospitals)\n",
    "    clinics = generate_clinics(num_clinics, num_hospitals)\n",
    "    providers = generate_providers(num_providers, num_hospitals, num_clinics)\n",
    "    patients = generate_patients(num_patients, providers)\n",
    "    appointments = generate_appointments(num_appointments, providers, patients)\n",
    "    medical_assets = generate_medical_assets(num_assets, num_hospitals, num_clinics)\n",
    "    workforce = generate_workforce(num_employees, num_hospitals)\n",
    "    case_allocations = generate_case_allocations(num_cases, providers, patients)\n",
    "\n",
    "    # Create data directory if not exists\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    hospitals.to_csv('data/hospitals.csv', index=False)\n",
    "    clinics.to_csv('data/clinics.csv', index=False)\n",
    "    providers.to_csv('data/providers.csv', index=False)\n",
    "    patients.to_csv('data/patients.csv', index=False)\n",
    "    appointments.to_csv('data/appointments.csv', index=False)\n",
    "    medical_assets.to_csv('data/medical_assets.csv', index=False)\n",
    "    workforce.to_csv('data/workforce.csv', index=False)\n",
    "    case_allocations.to_csv('data/case_allocations.csv', index=False)\n",
    "    print(\"Data generated successfully!\")\n",
    "\n",
    "# Generate data\n",
    "generate_data()\n",
    "\n",
    "# Test case script\n",
    "def test_read_data_files():\n",
    "    DATA_FILES = [\n",
    "        'data/hospitals.csv',\n",
    "        'data/clinics.csv',\n",
    "        'data/providers.csv',\n",
    "        'data/patients.csv',\n",
    "        'data/appointments.csv',\n",
    "        'data/medical_assets.csv',\n",
    "        'data/workforce.csv',\n",
    "        'data/case_allocations.csv'\n",
    "    ]\n",
    "\n",
    "    for file in DATA_FILES:\n",
    "        try:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            print(f\"Successfully read {file}\")\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file, encoding='latin-1', low_memory=False)\n",
    "            print(f\"Successfully read {file} with latin-1 encoding\")\n",
    "        \n",
    "        print(df.head(4))\n",
    "        \n",
    "        \n",
    "        # Check and convert data types if needed\n",
    "        for col in df.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]) and not pd.api.types.is_string_dtype(df[col]):\n",
    "                df[col] = df[col].astype(str)\n",
    "                print(f\"Column {col} in {file} converted to string\")\n",
    "\n",
    "# Run tests\n",
    "test_read_data_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Doctor Function to read data files and perform data cleaning and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file 'data/data_quality_checks_template.xlsx' created successfully with instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = [re.sub(r'\\W+', '_', col).lower() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def is_pii(column_name):\n",
    "    pii_keywords = [\"name\", \"dob\", \"date of birth\", \"age\", \"contact number\"]\n",
    "    for keyword in pii_keywords:\n",
    "        if similar(column_name.lower(), keyword) > 0.8:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def read_all_structured_files(directory_path):\n",
    "    all_files = glob.glob(os.path.join(directory_path, \"*.csv\")) + glob.glob(os.path.join(directory_path, \"*.xlsx\"))\n",
    "    all_sheets = []\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = clean_column_names(df)\n",
    "            all_sheets.append((file_path, df))\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            xls = pd.ExcelFile(file_path)\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                df = clean_column_names(df)\n",
    "                all_sheets.append((f\"{file_path} - {sheet_name}\", df))\n",
    "    \n",
    "    return all_sheets\n",
    "\n",
    "def find_critical_elements(all_sheets):\n",
    "    column_files_map = defaultdict(list)\n",
    "    for file_path, df in all_sheets:\n",
    "        for column in df.columns:\n",
    "            column_files_map[column].append(file_path)\n",
    "    \n",
    "    critical_elements = {column: files for column, files in column_files_map.items() if len(files) > 1}\n",
    "    return critical_elements\n",
    "\n",
    "def configure_quality_check(csv_file_path, excel_file_path=None):\n",
    "    all_sheets = read_all_structured_files(os.path.dirname(csv_file_path))\n",
    "    critical_elements = find_critical_elements(all_sheets)\n",
    "    \n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = clean_column_names(df)\n",
    "    column_names = df.columns.tolist()\n",
    "\n",
    "    pii_flags = []\n",
    "    critical_data_elements = []\n",
    "    \n",
    "    for column in column_names:\n",
    "        if is_pii(column):\n",
    "            similar_columns = [col for sheet in all_sheets for col in sheet[1].columns if similar(col.lower(), column.lower()) > 0.8]\n",
    "            description = ', '.join(set(similar_columns))\n",
    "            pii_flags.append(f\"Yes, description: {description}\")\n",
    "        else:\n",
    "            pii_flags.append(\"No\")\n",
    "        \n",
    "        if column in critical_elements:\n",
    "            critical_data_elements.append(f\"Yes, files: {', '.join(critical_elements[column])}\")\n",
    "        else:\n",
    "            critical_data_elements.append(\"No\")\n",
    "    \n",
    "    data_quality_checks_df = pd.DataFrame({\n",
    "        \"column_names\": column_names,\n",
    "        \"PII_Flag\": pii_flags,\n",
    "        \"test_completeness\": [\"Not Assessed\" for _ in column_names],  # Set default value to \"Not Assessed\"\n",
    "        \"test_uniqueness\": [\"\" for _ in column_names],\n",
    "        \"test_timeliness\": [\"\" for _ in column_names],\n",
    "        \"test_consistency\": [\"\" for _ in column_names],\n",
    "        \"test_accuracy\": [\"\" for _ in column_names],\n",
    "        \"test_validity\": [\"\" for _ in column_names],\n",
    "        \"critical_data_element\": critical_data_elements\n",
    "    })\n",
    "\n",
    "    if not excel_file_path:\n",
    "        excel_file_path = os.path.join(os.getcwd(), 'data_quality_checks_template.xlsx')\n",
    "\n",
    "    with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "        data_quality_checks_df.to_excel(writer, sheet_name='Data Quality Checks', index=False, startrow=1)\n",
    "\n",
    "    # Load the workbook to add the description\n",
    "    workbook = load_workbook(excel_file_path)\n",
    "    sheet = workbook['Data Quality Checks']\n",
    "    \n",
    "    # Add description at the top\n",
    "    description = (f\"File Name: {os.path.basename(csv_file_path)}\\n\"\n",
    "                   \"Please provide 'Yes' or 'No' in the columns below for each data quality check.\")\n",
    "    sheet['A1'] = description\n",
    "    sheet.merge_cells('A1:H1')\n",
    "    sheet['A1'].alignment = Alignment(wrap_text=True, vertical='center')\n",
    "\n",
    "    # Adjust column widths\n",
    "    for col in range(1, sheet.max_column + 1):\n",
    "        max_length = 0\n",
    "        column = get_column_letter(col)\n",
    "        for cell in sheet[column]:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2)\n",
    "        sheet.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    workbook.save(excel_file_path)\n",
    "    print(f\"Excel file '{excel_file_path}' created successfully with instructions.\")\n",
    "\n",
    "\n",
    "data_file_path = 'data/appointments.csv'\n",
    "template_file_path = 'data/data_quality_checks_template.xlsx'\n",
    "\n",
    "# Configure quality check and create template\n",
    "configure_quality_check(data_file_path, template_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data quality checks in the created Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness analysis results:\n",
      "        Column Name Total Rows Missing Values Non-Missing Values  \\\n",
      "0    appointment_id        500              0                500   \n",
      "1        patient_id        500              0                500   \n",
      "2       provider_id        500              0                500   \n",
      "3  appointment_date        500              0                500   \n",
      "4            reason        500              0                500   \n",
      "5           waiting        500              0                500   \n",
      "6      waiting_time        500              0                500   \n",
      "\n",
      "   Completeness (%)  \n",
      "0             100.0  \n",
      "1             100.0  \n",
      "2             100.0  \n",
      "3             100.0  \n",
      "4             100.0  \n",
      "5             100.0  \n",
      "6             100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80697/2353177093.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  completeness_results = pd.concat([completeness_results, completeness_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Total Rows</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Non-Missing Values</th>\n",
       "      <th>Completeness (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appointment_id</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient_id</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>provider_id</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appointment_date</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reason</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>waiting</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>waiting_time</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Column Name Total Rows Missing Values Non-Missing Values  \\\n",
       "0    appointment_id        500              0                500   \n",
       "1        patient_id        500              0                500   \n",
       "2       provider_id        500              0                500   \n",
       "3  appointment_date        500              0                500   \n",
       "4            reason        500              0                500   \n",
       "5           waiting        500              0                500   \n",
       "6      waiting_time        500              0                500   \n",
       "\n",
       "   Completeness (%)  \n",
       "0             100.0  \n",
       "1             100.0  \n",
       "2             100.0  \n",
       "3             100.0  \n",
       "4             100.0  \n",
       "5             100.0  \n",
       "6             100.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = [re.sub(r'\\W+', '_', col).lower() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def read_data_quality_template(excel_file_path):\n",
    "    df_template = pd.read_excel(excel_file_path, sheet_name='Data Quality Checks', skiprows=1)\n",
    "    return df_template\n",
    "\n",
    "def assess_completeness(df, column_name):\n",
    "    total_rows = len(df)\n",
    "    missing_values = df[column_name].isnull().sum()\n",
    "    non_missing_values = total_rows - missing_values\n",
    "    completeness_percentage = (non_missing_values / total_rows) * 100\n",
    "\n",
    "    completeness_df = pd.DataFrame({\n",
    "        'Column Name': [column_name],\n",
    "        'Total Rows': [total_rows],\n",
    "        'Missing Values': [missing_values],\n",
    "        'Non-Missing Values': [non_missing_values],\n",
    "        'Completeness (%)': [round(completeness_percentage, 2)]\n",
    "    })\n",
    "\n",
    "    return completeness_df\n",
    "\n",
    "def evaluate_data_quality(data_file_path, template_file_path):\n",
    "    df_template = read_data_quality_template(template_file_path)\n",
    "\n",
    "    if data_file_path.endswith('.csv'):\n",
    "        df_data = pd.read_csv(data_file_path)\n",
    "    elif data_file_path.endswith('.xlsx'):\n",
    "        df_data = pd.read_excel(data_file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use .csv or .xlsx files.\")\n",
    "\n",
    "    completeness_results = pd.DataFrame(columns=['Column Name', 'Total Rows', 'Missing Values', 'Non-Missing Values', 'Completeness (%)'])\n",
    "\n",
    "    if not df_data.empty:\n",
    "        for index, row in df_template.iterrows():\n",
    "            column_name = row['column_names']\n",
    "            test_completeness = str(row['test_completeness']).strip().lower() if pd.notna(row['test_completeness']) else 'not assessed'\n",
    "            if test_completeness == 'yes':\n",
    "                if column_name in df_data.columns:\n",
    "                    completeness_df = assess_completeness(df_data, column_name)\n",
    "                    if not completeness_df.empty:\n",
    "                        completeness_results = pd.concat([completeness_results, completeness_df], ignore_index=True)\n",
    "                else:\n",
    "                    print(f\"Warning: Column '{column_name}' not found in data file.\")\n",
    "            else:\n",
    "                not_assessed_df = pd.DataFrame({\n",
    "                    'Column Name': [column_name],\n",
    "                    'Total Rows': ['N/A'],\n",
    "                    'Missing Values': ['N/A'],\n",
    "                    'Non-Missing Values': ['N/A'],\n",
    "                    'Completeness (%)': ['Not Assessed']\n",
    "                })\n",
    "                completeness_results = pd.concat([completeness_results, not_assessed_df], ignore_index=True)\n",
    "    else:\n",
    "        print(\"Warning: The data file is empty.\")\n",
    "\n",
    "    if completeness_results.empty:\n",
    "        print(\"No completeness analysis results to display.\")\n",
    "    else:\n",
    "        print(\"Completeness analysis results:\")\n",
    "        print(completeness_results)\n",
    "\n",
    "    return completeness_results\n",
    "\n",
    "# Example usage:\n",
    "data_file_path = 'data/appointments.csv'\n",
    "template_file_path = 'data/data_quality_checks_template.xlsx'\n",
    "\n",
    "# Evaluate data quality based on the template\n",
    "completeness_results = evaluate_data_quality(data_file_path, template_file_path)\n",
    "\n",
    "# Display the completeness results DataFrame\n",
    "# print(\"Completeness Results DataFrame:\")\n",
    "# print(completeness_results)\n",
    "completeness_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
